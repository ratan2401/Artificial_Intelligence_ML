{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS2103 / Lab-02 - Assignment 03 - `04-08-2025`\n",
    "\n",
    "**Topic**: Gradient Descent\n",
    "\n",
    "**Instructions**: Complete all the tasks from one of *section A or B* below. Submit your code as a notebook file named `A03.ipynb`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A: Python Programming\n",
    "\n",
    "Use pytorch to create two perceptron as shown in the figure below:\n",
    "\n",
    "[1] ![F1](http://172.30.1.73:2103/p/F1.png)\n",
    "\n",
    "[2] ![F2](http://172.30.1.73:2103/p/F2.png)\n",
    "\n",
    "\n",
    "[a] Assign proper weights to the neural network to show logical functions like AND, OR and XOR.\n",
    "\n",
    "[b] Use Autograd to perform gradient descent on the Neural Netowrk Models created above. hint: generate your own dataset by assuming some ground truth about the weights.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B: Gradient Descent\n",
    "\n",
    "* **Linear Regression**: You are provided with two datasets: [car_train.csv](http://172.30.1.73:2103/p/car_train.csv) and [car_test.csv](http://172.30.1.73:2103/p/car_test.csv) Your task is to predict car prices (price) based on the given features using linear regression. Report the loss observed by the trained model on the test set. Also, plot the ground truth and predictions for the test set. \n",
    "\n",
    "* **Logistic Regression**: You are provided with the [Breat Cancer Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data) containing 30 features for each data point. Convert the diagnosis column to binary labels (1 for Malignant, 0 for Benign). Split the data into training and testing sets. Perform Logistic Regression Classification to train a simple neural network model. Report the Accuracy, Precision and Recall on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section A: Perceptron Models for AND, OR, XOR using PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Dataset for logical functions (inputs: [x1, x2], outputs: AND, OR, XOR)\n",
    "X = torch.tensor([[0,0],[0,1],[1,0],[1,1]], dtype=torch.float32)\n",
    "Y_and = torch.tensor([[0],[0],[0],[1]], dtype=torch.float32)\n",
    "Y_or  = torch.tensor([[0],[1],[1],[1]], dtype=torch.float32)\n",
    "Y_xor = torch.tensor([[0],[1],[1],[0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Perceptron outputs: [0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# 1. Single-layer Perceptron for AND (can be solved with 1 neuron)\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.fc(x))\n",
    "\n",
    "# Assign weights manually for AND\n",
    "and_perceptron = Perceptron()\n",
    "with torch.no_grad():\n",
    "    and_perceptron.fc.weight[:] = torch.tensor([[1.0, 1.0]])\n",
    "    and_perceptron.fc.bias[:] = torch.tensor([-1.5])\n",
    "print('AND Perceptron outputs:', and_perceptron(X).round().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR Perceptron outputs: [0.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# 2. Single-layer Perceptron for OR (can be solved with 1 neuron)\n",
    "or_perceptron = Perceptron()\n",
    "with torch.no_grad():\n",
    "    or_perceptron.fc.weight[:] = torch.tensor([[1.0, 1.0]])\n",
    "    or_perceptron.fc.bias[:] = torch.tensor([-0.5])\n",
    "print('OR Perceptron outputs:', or_perceptron(X).round().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Perceptron outputs: [1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# 3. Two-layer Perceptron for XOR (cannot be solved with single layer)\n",
    "class XORNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Assign weights manually for XOR (from known solution)\n",
    "xor_net = XORNet()\n",
    "with torch.no_grad():\n",
    "    xor_net.fc1.weight[:] = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\n",
    "    xor_net.fc1.bias[:] = torch.tensor([ -0.5, -1.5])\n",
    "    xor_net.fc2.weight[:] = torch.tensor([[1.0, -2.0]])\n",
    "    xor_net.fc2.bias[:] = torch.tensor([0.5])\n",
    "print('XOR Perceptron outputs:', xor_net(X).round().view(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use autograd to train the models (gradient descent)\n",
    "def train(model, X, Y, epochs=2000, lr=0.1):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained AND: [0.0, 0.0, 0.0, 1.0]\n",
      "Trained OR: [0.0, 1.0, 1.0, 1.0]\n",
      "Trained OR: [0.0, 1.0, 1.0, 1.0]\n",
      "Trained XOR: [0.0, 1.0, 0.0, 1.0]\n",
      "Trained XOR: [0.0, 1.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# 5. Train perceptrons and show results using autograd\n",
    "# Train AND perceptron\n",
    "and_model = Perceptron()\n",
    "train(and_model, X, Y_and)\n",
    "print('Trained AND:', and_model(X).round().view(-1).tolist())\n",
    "\n",
    "# Train OR perceptron\n",
    "or_model = Perceptron()\n",
    "train(or_model, X, Y_or)\n",
    "print('Trained OR:', or_model(X).round().view(-1).tolist())\n",
    "\n",
    "# Train XOR network\n",
    "xor_model = XORNet()\n",
    "train(xor_model, X, Y_xor)\n",
    "print('Trained XOR:', xor_model(X).round().view(-1).tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
