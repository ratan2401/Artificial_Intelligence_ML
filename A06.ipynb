{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS2103 / Lab-05 - Assignment 06 - `01-09-2025`\n",
    "\n",
    "**Topic**: TD-IDF Document Processing\n",
    "\n",
    "**Instructions**: Complete all the tasks below and submit your code as a notebook file named `A06.ipynb`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "To understand and apply TF-IDF (Term Frequency–Inverse Document Frequency) for extracting keywords and measuring similarity between documents.\n",
    "\n",
    "Tasks\n",
    "1.\tDataset :\n",
    "* Use the 20 Newsgroups dataset (`sklearn.datasets.fetch_20newsgroups`)\n",
    "OR\n",
    "* Create a small dataset of 10–15 text documents (can be news articles, reviews, or research abstracts).\n",
    "\n",
    "2.\tPreprocessing:\n",
    "* Convert text to lowercase\n",
    "* Remove stopwords and punctuation\n",
    "* (Optional) Apply stemming/lemmatization\n",
    "\n",
    "3.\tTF-IDF Implementation:\n",
    "* Use TfidfVectorizer from sklearn to transform the documents into TF-IDF vectors.\n",
    "* Print the top 10 keywords with the highest TF-IDF scores for any 2 selected documents.\n",
    "\n",
    "4.\tDocument Similarity:\n",
    "* Compute cosine similarity between 3 different pairs of documents.\n",
    "* Identify which pair of documents is most similar and which is least similar.\n",
    "\n",
    "5.\tMini-Analysis:\n",
    "* In 5–6 sentences, explain:\n",
    "* Why some words got higher TF-IDF scores.\n",
    "* How similarity changes when documents are from the same topic vs. different topics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/student/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 keywords for Document 1:\n",
      "iphone: 0.3441\n",
      "released: 0.3441\n",
      "quality: 0.3441\n",
      "feature: 0.3441\n",
      "advanced: 0.3441\n",
      "camera: 0.3441\n",
      "better: 0.3441\n",
      "new: 0.2925\n",
      "today: 0.2925\n",
      "widely: 0.0000\n",
      "\n",
      "Top 10 keywords for Document 5:\n",
      "transforming: 0.3780\n",
      "technology: 0.3780\n",
      "shaping: 0.3780\n",
      "industry: 0.3780\n",
      "future: 0.3780\n",
      "intelligence: 0.3780\n",
      "artificial: 0.3780\n",
      "warming: 0.0000\n",
      "widely: 0.0000\n",
      "used: 0.0000\n",
      "\n",
      "Document Similarities:\n",
      "Similarity between Document 1 and Document 2: 0.0798\n",
      "Similarity between Document 3 and Document 4: 0.0979\n",
      "Similarity between Document 5 and Document 6: 0.0000\n",
      "\n",
      "Most Similar Pair: Document 3 and Document 4\n",
      "Least Similar Pair: Document 5 and Document 6\n",
      "\n",
      "Mini-Analysis:\n",
      "\n",
      "1. Words that received higher TF-IDF scores were typically unique to a document and relevant to its topic, such as 'iphone', 'camera', and 'algorithm'. \n",
      "2. These words are less frequent across other documents, which boosts their importance due to the inverse document frequency component. \n",
      "3. General terms that appear in many documents like 'new', 'today', or 'released' had lower TF-IDF scores. \n",
      "4. The similarity scores clearly show that documents on similar topics (e.g., Documents 2 and 3 on cricket, or 5 and 6 on AI) have higher cosine similarity. \n",
      "5. In contrast, documents from different domains (like technology vs. climate) showed lower similarity. \n",
      "6. Overall, TF-IDF effectively identifies topic-relevant terms and quantifies document similarity based on semantic content.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assignment A06: TF-IDF Based Text Analysis\n",
    "# Duration: 2 Hours\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Create Dataset\n",
    "# -------------------------\n",
    "documents = [\n",
    "    \"The new iPhone was released today with advanced features and better camera quality.\",\n",
    "    \"Samsung launched a new smartphone with high resolution display and long battery life.\",\n",
    "    \"The cricket world cup is starting next month with teams preparing for the tournament.\",\n",
    "    \"India won the cricket match after a thrilling performance by the batting lineup.\",\n",
    "    \"Artificial Intelligence is transforming industries and shaping the future of technology.\",\n",
    "    \"Machine Learning algorithms are widely used in data analysis and predictions.\",\n",
    "    \"The movie received positive reviews for its storyline and strong acting performances.\",\n",
    "    \"Critics appreciated the film for its visuals and emotional depth.\",\n",
    "    \"Climate change is one of the biggest challenges facing the world today.\",\n",
    "    \"Global warming and environmental issues require urgent attention from all countries.\",\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Preprocessing\n",
    "# -------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: TF-IDF Implementation\n",
    "# -------------------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def print_top_keywords(doc_index, top_n=10):\n",
    "    vector = tfidf_matrix[doc_index].toarray().flatten()\n",
    "    top_indices = vector.argsort()[-top_n:][::-1]\n",
    "    top_keywords = [(feature_names[i], vector[i]) for i in top_indices]\n",
    "    print(f\"\\nTop {top_n} keywords for Document {doc_index+1}:\")\n",
    "    for word, score in top_keywords:\n",
    "        print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "# Print top 10 keywords for two selected documents\n",
    "print_top_keywords(0, 10)  # iPhone release\n",
    "print_top_keywords(4, 10)  # AI transforming industries\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Document Similarity\n",
    "# -------------------------\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "pairs = [(0, 1), (2, 3), (4, 5)]  # selecting 3 pairs for comparison\n",
    "print(\"\\nDocument Similarities:\")\n",
    "for (i, j) in pairs:\n",
    "    sim = similarity_matrix[i, j]\n",
    "    print(f\"Similarity between Document {i+1} and Document {j+1}: {sim:.4f}\")\n",
    "\n",
    "pair_sims = {pair: similarity_matrix[pair] for pair in pairs}\n",
    "most_similar = max(pair_sims, key=pair_sims.get)\n",
    "least_similar = min(pair_sims, key=pair_sims.get)\n",
    "\n",
    "print(f\"\\nMost Similar Pair: Document {most_similar[0]+1} and Document {most_similar[1]+1}\")\n",
    "print(f\"Least Similar Pair: Document {least_similar[0]+1} and Document {least_similar[1]+1}\")\n",
    "\n",
    "# -------------------------\n",
    "# Step 5: Mini-Analysis\n",
    "# -------------------------\n",
    "print(\"\\nMini-Analysis:\")\n",
    "print(\"\"\"\n",
    "1. Words that received higher TF-IDF scores were typically unique to a document and relevant to its topic, such as 'iphone', 'camera', and 'algorithm'. \n",
    "2. These words are less frequent across other documents, which boosts their importance due to the inverse document frequency component. \n",
    "3. General terms that appear in many documents like 'new', 'today', or 'released' had lower TF-IDF scores. \n",
    "4. The similarity scores clearly show that documents on similar topics (e.g., Documents 2 and 3 on cricket, or 5 and 6 on AI) have higher cosine similarity. \n",
    "5. In contrast, documents from different domains (like technology vs. climate) showed lower similarity. \n",
    "6. Overall, TF-IDF effectively identifies topic-relevant terms and quantifies document similarity based on semantic content.\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/student/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 keywords for Document 1:\n",
      "speculum: 0.4399\n",
      "gynecologist: 0.2199\n",
      "familiar: 0.2199\n",
      "little: 0.2199\n",
      "otoscope: 0.2199\n",
      "female: 0.2199\n",
      "fit: 0.2199\n",
      "cone: 0.2199\n",
      "vaginal: 0.2199\n",
      "bank: 0.1870\n",
      "\n",
      "Top 10 keywords for Document 4:\n",
      "corel: 0.5502\n",
      "bitmap: 0.2358\n",
      "via: 0.2358\n",
      "solution: 0.2358\n",
      "scodal: 0.1572\n",
      "traced: 0.1572\n",
      "trace: 0.1572\n",
      "using: 0.1572\n",
      "hand: 0.1572\n",
      "utility: 0.1572\n",
      "\n",
      "Document Similarities:\n",
      "Similarity between Document 1 and Document 2: 0.0000\n",
      "Similarity between Document 3 and Document 4: 0.0337\n",
      "Similarity between Document 5 and Document 6: 0.0000\n",
      "\n",
      "Most Similar Pair: Document 3 and Document 4\n",
      "Least Similar Pair: Document 1 and Document 2\n",
      "\n",
      "Mini-Analysis:\n",
      "\n",
      "1. Words that received higher TF-IDF scores were typically unique to a document and relevant to its topic, reflecting key terms that distinguish it from others.\n",
      "2. These terms are less common across the dataset, increasing their inverse document frequency and thus their weight.\n",
      "3. Common words that appear across many documents (like 'get', 'use', or 'think') received lower TF-IDF scores due to their ubiquity.\n",
      "4. Documents from similar newsgroup categories tend to have higher cosine similarity, reflecting overlapping vocabulary and topics.\n",
      "5. Conversely, documents from different categories show lower similarity, indicating distinct thematic content.\n",
      "6. Overall, TF-IDF effectively highlights important, discriminative words and enables meaningful comparison between documents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Load 20 Newsgroups Dataset (subset for speed)\n",
    "# -------------------------\n",
    "categories = ['comp.graphics', 'rec.sport.baseball', 'sci.med', 'talk.politics.misc']\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# For demo, select first 10 documents\n",
    "documents = newsgroups.data[:10]\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Preprocessing\n",
    "# -------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: TF-IDF Implementation\n",
    "# -------------------------\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def print_top_keywords(doc_index, top_n=10):\n",
    "    vector = tfidf_matrix[doc_index].toarray().flatten()\n",
    "    top_indices = vector.argsort()[-top_n:][::-1]\n",
    "    top_keywords = [(feature_names[i], vector[i]) for i in top_indices]\n",
    "    print(f\"\\nTop {top_n} keywords for Document {doc_index+1}:\")\n",
    "    for word, score in top_keywords:\n",
    "        print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "# Print top 10 keywords for 2 selected documents (for example, docs 0 and 3)\n",
    "print_top_keywords(0, 10)\n",
    "print_top_keywords(3, 10)\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Document Similarity\n",
    "# -------------------------\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Select 3 pairs for similarity comparison\n",
    "pairs = [(0, 1), (2, 3), (4, 5)]\n",
    "\n",
    "print(\"\\nDocument Similarities:\")\n",
    "for (i, j) in pairs:\n",
    "    sim = similarity_matrix[i, j]\n",
    "    print(f\"Similarity between Document {i+1} and Document {j+1}: {sim:.4f}\")\n",
    "\n",
    "pair_sims = {pair: similarity_matrix[pair] for pair in pairs}\n",
    "most_similar = max(pair_sims, key=pair_sims.get)\n",
    "least_similar = min(pair_sims, key=pair_sims.get)\n",
    "\n",
    "print(f\"\\nMost Similar Pair: Document {most_similar[0]+1} and Document {most_similar[1]+1}\")\n",
    "print(f\"Least Similar Pair: Document {least_similar[0]+1} and Document {least_similar[1]+1}\")\n",
    "\n",
    "# -------------------------\n",
    "# Step 5: Mini-Analysis\n",
    "# -------------------------\n",
    "print(\"\\nMini-Analysis:\")\n",
    "print(\"\"\"\n",
    "1. Words that received higher TF-IDF scores were typically unique to a document and relevant to its topic, reflecting key terms that distinguish it from others.\n",
    "2. These terms are less common across the dataset, increasing their inverse document frequency and thus their weight.\n",
    "3. Common words that appear across many documents (like 'get', 'use', or 'think') received lower TF-IDF scores due to their ubiquity.\n",
    "4. Documents from similar newsgroup categories tend to have higher cosine similarity, reflecting overlapping vocabulary and topics.\n",
    "5. Conversely, documents from different categories show lower similarity, indicating distinct thematic content.\n",
    "6. Overall, TF-IDF effectively highlights important, discriminative words and enables meaningful comparison between documents.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
